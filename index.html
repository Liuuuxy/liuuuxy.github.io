<!DOCTYPE html>


<html lang="zh-Hans">


<head>
  <meta name="baidu-site-verification" content="code-Tu1khIKhGS" />
  <meta name="google-site-verification" content="nZhURXzv_ceGCrYIIHuRYG_MLifbE7Waif3vUJ08Axs" />
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     Dream
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/bitbug_favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link rel="alternate" href="/atom.xml" title="Dream" type="application/atom+xml">
</head>

</html>

<body>
  <!-- 漂亮雪花 -->
<script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"></script>
<script type="text/javascript" src="/js/snow.js"></script>
<style type="text/css">
.snow-container{
position:fixed;
top:0;left:0;
width:100%;
height:100%;
pointer-events:none;
z-index:100001;
}
</style>
<div class="snow-container"></div>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <!-- 从这里开始，修改代码 -->
    <div class="bg-box">
      <img src="" alt="" id="cover-pic" />
    </div>
    <script>
      $(document).ready(function(){
        var i=Math.floor((Math.random()*4));
        imgs=[ "/images/mycover1.jpg", "/images/mycover2.jpg", "/images/mycover.jpg", "/images/mycover3.jpg",]
        pic=document.getElementById("cover-pic");
        pic.src=imgs[i];
      })
      </script>
    <!-- 到这里结束 -->
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Dream</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['Take pleasure seriously.', 'Seven times have I despised at my soul.', 'Everyone you meet is fighting a battle you know nothing about. Be kind. Always.'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-RCNN"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/12/27/RCNN/"
    >Region-Based Convolutional Neural Network</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/12/27/RCNN/" class="article-date">
  <time datetime="2020-12-27T14:16:29.542Z" itemprop="datePublished">2020-12-27</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="Region-Based-Convolutional-Neural-Network"><a href="#Region-Based-Convolutional-Neural-Network" class="headerlink" title="Region-Based Convolutional Neural Network"></a>Region-Based Convolutional Neural Network</h2><p>RCNN</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-06-33.png" alt="img"></p>
<h3 id="Problems-with-RCNN"><a href="#Problems-with-RCNN" class="headerlink" title="Problems with RCNN"></a>Problems with RCNN</h3><p> Training an RCNN model is expensive and slow thanks to the below steps:</p>
<ul>
<li>Extracting 2,000 regions for each image based on selective search</li>
<li>Extracting features using CNN for every image region. Suppose we have N images, then the number of CNN features will be N*2,000</li>
<li>The entire process of object detection using RCNN has three models:<ol>
<li>CNN for feature extraction</li>
<li>Linear SVM classifier for identifying objects</li>
<li>Regression model for tightening the bounding boxes.</li>
</ol>
</li>
</ul>
<p>All these processes combine to make RCNN very slow. It takes around 40-50 seconds to make predictions for each new image, which essentially makes the model cumbersome and practically impossible to build when faced with a gigantic dataset.</p>
<h2 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h2><p>An immediate descendant to R-CNN is <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.08083.pdf"><strong>Fast R-CNN</strong></a>, which improves the detection speed through 2 augmentations: 1) Performing feature extraction before proposing regions, thus only running one CNN over the entire image, and 2) Replacing SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model.</p>
<ol>
<li>As with the earlier two techniques, we take an image as an input.</li>
<li>This image is passed to a ConvNet which in turns generates the Regions of Interest.</li>
<li>A RoI pooling layer is applied on all of these regions to reshape them as per the input of the ConvNet. Then, each region is passed on to a fully connected network.</li>
<li>A softmax layer is used on top of the fully connected network to output classes. Along with the softmax layer, a linear regression layer is also used parallely to output bounding box coordinates for predicted classes.</li>
</ol>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-47-18.png" alt="img"></p>
<h3 id="Problems-with-Fast-RCNN"><a href="#Problems-with-Fast-RCNN" class="headerlink" title="Problems with Fast RCNN"></a>Problems with Fast RCNN</h3><p>It also uses selective search as a proposal method to find the Regions of Interest, which is a slow and time consuming process. </p>
<h2 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h2><ol>
<li>We take an image as input and pass it to the ConvNet which returns the feature map for that image.</li>
<li>Region proposal network is applied on these feature maps. This returns the object proposals along with their objectness score.</li>
<li>A RoI pooling layer is applied on these proposals to bring down all the proposals to the same size.</li>
<li>Finally, the proposals are passed to a fully connected layer which has a softmax layer and a linear regression layer at its top, to classify and output the bounding boxes for objects.</li>
</ol>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-09-14-15-36.png" alt="img"></p>
<p>Faster RCNN:CNN-&gt;feature maps-&gt;Region Proposal Network(RPN)</p>
<p>RPN: Use a sliding window over these feature maps, and at each window, it generates <em>k</em> Anchor boxes of different shapes and sizes. RPN predicts the probability that an anchor is an object(do not consider what it exactly is), and a bounding box regressor for adjusting the anchor to better fit the object.</p>
<h3 id="Problems-with-Faster-RCNN"><a href="#Problems-with-Faster-RCNN" class="headerlink" title="Problems with Faster RCNN"></a>Problems with Faster RCNN</h3><ul>
<li>The algorithm requires many passes through a single image to extract all the objects</li>
<li>As there are different systems working one after the other, the performance of the systems further ahead depends on how the previous systems performed</li>
</ul>
<h2 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h2><table>
<thead>
<tr>
<th><strong>Algorithm</strong></th>
<th><strong>Features</strong></th>
<th><strong>Prediction time / image</strong></th>
<th><strong>Limitations</strong></th>
</tr>
</thead>
<tbody><tr>
<td>CNN</td>
<td>Divides the image into multiple regions and then classify each region into various classes.</td>
<td>–</td>
<td>Needs a lot of regions to predict accurately and hence high computation time.</td>
</tr>
<tr>
<td>RCNN</td>
<td>Uses selective search to generate regions. Extracts around 2000 regions from each image.</td>
<td>40-50 seconds</td>
<td>High computation time as each region is passed to the CNN separately also it uses three different model for making predictions.</td>
</tr>
<tr>
<td>Fast RCNN</td>
<td>Each image is passed only once to the CNN and feature maps are extracted. Selective search is used on these maps to generate predictions. Combines all the three models used in RCNN together.</td>
<td>2 seconds</td>
<td>Selective search is slow and hence computation time is still high.</td>
</tr>
<tr>
<td>Faster RCNN</td>
<td>Replaces the selective search method with region proposal network which made the algorithm much faster.</td>
<td>0.2 seconds</td>
<td>Object proposal takes time and as there are different systems working one after the other, the performance of systems depends on how the previous system has performed.</td>
</tr>
</tbody></table>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RCNN/" rel="tag">RCNN</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-PyTorch Tutorial Note"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/12/18/PyTorch%20Tutorial%20Note/"
    >PyTorch Tutorial Note</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/12/18/PyTorch%20Tutorial%20Note/" class="article-date">
  <time datetime="2020-12-18T11:09:32.000Z" itemprop="datePublished">2020-12-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h5 id="DEEP-LEARNING-WITH-PYTORCH-A-60-MINUTE-BLITZ"><a href="#DEEP-LEARNING-WITH-PYTORCH-A-60-MINUTE-BLITZ" class="headerlink" title="DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ"></a>DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a></p>
<p>一个官方的tutorials。分为四个部分：<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py">What is PyTorch?</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">Autograd: Automatic Differentiation</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py">Neural Networks</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">Training a Classifier</a>。可以在colab上运行。</p>
<h4 id="What-is-PyTorch？"><a href="#What-is-PyTorch？" class="headerlink" title="What is PyTorch？"></a>What is PyTorch？</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.rand(<span class="number">5</span>,<span class="number">3</span>)<span class="comment">#小数点后四位</span></span><br><span class="line">torch.randn_like(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.tensor(data, *, dtype=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, pin_memory=<span class="literal">False</span>) → Tensor<span class="comment">#小数点后四位</span></span><br></pre></td></tr></table></figure>
<h5 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
<h6 id="Addition"><a href="#Addition" class="headerlink" title="Addition"></a>Addition</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x+y</span><br><span class="line">torch.add(x,y)</span><br><span class="line">torch.add(x,y,out=result)</span><br><span class="line">y.add_(x)<span class="comment">#add x to y</span></span><br></pre></td></tr></table></figure>
<p>Any operation that mutates a tensor in-place is post-fixed with an <code>_</code>. For example: <code>x.copy_(y)</code>, <code>x.t_()</code>, will change <code>x</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.view(<span class="number">2</span>,<span class="number">8</span>)<span class="comment">#to reshape tensor</span></span><br></pre></td></tr></table></figure>

<h5 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h5><p>The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other.</p>
<p>torch.Tensor.item(): item() → number. Returns the value of this tensor as a standard Python number. This only works for tensors with one element. </p>
<h4 id="AutoGrad"><a href="#AutoGrad" class="headerlink" title="AutoGrad"></a>AutoGrad</h4><p><em>Two vital class: Tensor, Function.</em></p>
<p>Your backprop is defined by how your code is run, and that every single iteration can be different.</p>
<p>If you set its attribute <code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. <u>When you finish your computation you can call <code>.backward()</code> and have all the gradients computed automatically.</u> The gradient for this tensor will be accumulated into <code>.grad</code> attribute.</p>
<p>To stop a tensor from tracking history, you can call <code>.detach()</code> to detach it from the computation history, and to prevent future computation from being tracked, or wrapping the code block in <code>with torch.no_grad()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(<span class="built_in">input</span>, other, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure>

<p>Computes element-wise equality</p>
<p>The second argument can be a number or a tensor whose shape is <a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eq(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>

<h4 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h4><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a></p>
<p>An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code>\ that returns the <code>output</code>.</p>
<p><code>torch.nn</code> only supports mini-batches. The entire <code>torch.nn</code> package only supports inputs that are a mini-batch of samples, and not a single sample.</p>
<p>For example, <code>nn.Conv2d</code> will take in a 4D Tensor of<br><code>nSamples x nChannels x Height x Width</code>.</p>
<p>If you have a single sample, just use <code>input.unsqueeze(0)</code> to add<br>a fake batch dimension.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">Conv2d</span>(<span class="params">in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = <span class="number">1</span>, padding: Union[T, Tuple[T, T]] = <span class="number">0</span>, dilation: Union[T, Tuple[T, T]] = <span class="number">1</span>, groups: <span class="built_in">int</span> = <span class="number">1</span>, bias: <span class="built_in">bool</span> = <span class="literal">True</span>, padding_mode: <span class="built_in">str</span> = <span class="string">&#x27;zeros&#x27;</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line">self.conv1 = nn.Conv2d(1, 6, 3)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">Linear</span>(<span class="params">in_features: <span class="built_in">int</span>, out_features: <span class="built_in">int</span>, bias: <span class="built_in">bool</span> = <span class="literal">True</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"># <span class="title">an</span> <span class="title">affine</span> <span class="title">operation</span>:</span> y = Wx + b</span><br><span class="line">self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically registered as a parameter when assigned as an attribute to a</em> <code>Module</code>.</li>
</ul>
<h5 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h5><p> a graph of computations that looks like this:::</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d   </span><br><span class="line">	  -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear   </span><br><span class="line">	  -&gt; MSELoss   </span><br><span class="line">	  -&gt; loss</span><br></pre></td></tr></table></figure>

<p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has <code>requires_grad=True</code> will have their <code>.grad</code> Tensor accumulated with the gradient. </p>
<p>For illustration, let us follow a few steps backward:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn) <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>

<h5 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h5><p><code>loss.backward()</code>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.</p>
<h5 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h5><p><code>torch.optim</code> implements all these methods: SGD, Nesterov-SGD, Adam, RMSProp, etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<h4 id="Training-a-Classifier"><a href="#Training-a-Classifier" class="headerlink" title="Training a Classifier"></a>Training a Classifier</h4><p><code>torchvision</code> has data loaders for common datasets such as Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz., <code>torchvision.datasets</code> and <code>torch.utils.data.DataLoader</code>.</p>
<p>torch.utils.data.DataLoader: <a target="_blank" rel="noopener" href="https://devdocs.io/pytorch/data#torch.utils.data.DataLoader">https://devdocs.io/pytorch/data#torch.utils.data.DataLoader</a></p>
<p>Saving and loading models: <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/serialization.html">https://pytorch.org/docs/stable/notes/serialization.html</a></p>
<p>loading from an iterable-style dataset is roughly equivalent with:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line"><span class="keyword">for</span> indices <span class="keyword">in</span> batch_sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn([<span class="built_in">next</span>(dataset_iter) <span class="keyword">for</span> _ <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>

<p><code>next()</code> calls the <code>__next__()</code> method on that iterator to get the first iteration. Running <code>next()</code> again will get the second item of the iterator, etc.</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-2020-11-22-微机复习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/11/22/2020-11-22-%E5%BE%AE%E6%9C%BA%E5%A4%8D%E4%B9%A0/"
    >微机复习</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/11/22/2020-11-22-%E5%BE%AE%E6%9C%BA%E5%A4%8D%E4%B9%A0/" class="article-date">
  <time datetime="2020-11-22T15:43:01.000Z" itemprop="datePublished">2020-11-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Review/">Review</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="第2章-8088"><a href="#第2章-8088" class="headerlink" title="第2章 8088"></a>第2章 8088</h2><p>锁存器74HC373：8D三态锁存器 LE=1 Q=D LE=0 Q锁存D</p>
<p>74LS273/74HC273 具有异步清零的 上升沿锁存器</p>
<p>74LS244 缓冲器(实质是三态开关 8位单向缓冲器 双4位分两组 输出与输入同相</p>
<p>74LS245缓冲器(实质是三态开关 8位双向缓冲器 控制端E低电平有效 输出与输入同相 DIR：决定传输方向 DIR=1：A → B DIR=0：B → A</p>
<p>Intel 8286 缓冲器(实质是三态开关) 8位双向缓冲器 控制端OE低电平有效 输出与输入同相</p>
<p>3-8译码器:74LS138 低电平有效输出 使能端S S1高有效S23低</p>
<h5 id="8088的外部引脚"><a href="#8088的外部引脚" class="headerlink" title="8088的外部引脚"></a>8088的外部引脚</h5><p>FFFF0H放第一条指令。</p>
<p>端口/存储器选择信号IO/M（低）</p>
<p>对端口读或写， IO/M输出高电平 <strong>IN</strong> AL ，25H ；读I/O <strong>OUT</strong> 52H，AL ；写I/O</p>
<p>DT/R(低)    数据传送/接收信号 (出，三态)</p>
<p>AD7～AD0数据和地址复用 先传送地址，后传送数据。 地址20位 数据8位</p>
<p>ALE(25)地址锁存信号(出，三态)  当A19～0出现地址，产生正脉冲。  利用该脉冲锁存地址</p>
<p>DEN(26)数据允许 (出，三态)  低电平：AD7~0传送的是数据。</p>
<p>A19/S6-A16/S3仅在T1输出地址,其他时 间输出状态S6-S3（分时复用）</p>
<h5 id="8088的两种工作模式"><a href="#8088的两种工作模式" class="headerlink" title="8088的两种工作模式"></a>8088的两种工作模式</h5><p>最大：多处理器系统    最小：单处理器系统</p>
<p>两种模式利用MN/MX引脚区别</p>
<hr>
<p>指令周期 &gt; 总线周期 &gt; 时钟周期</p>
<ul>
<li><p>时钟周期（Clock Cycle） 时钟的周期，时钟频率的倒数。</p>
</li>
<li><p>总线周期（ Bus Cycle ） CPU通过总线与外部进行基本操作（一次数据交换） 的过程 I/O读或写总线周期，存储器读或写总线周期,… （M/IO读写）</p>
</li>
<li><p>指令周期（ Instruction Cycle ） 取指、译码、读写操作数到执行完成的过程</p>
<p>总线周期需要4个时钟周期</p>
<p>空闲周期Ti ：空转，2个总线周期之间插入。</p>
<p>等待周期Tw：用于延长总线周期，插入T3和T4之间</p>
</li>
</ul>
<p>–CPU在T3期间采样READY信号</p>
<p>1.T3期间检测READY是否有效？ </p>
<p>2.如果READY无效，在T3和T4间 插一个等效T3的Tw状态，转1 </p>
<p>3.如果READY有效，执行完T3后， 进入T4。</p>
<p><strong>存储器写总线周期</strong></p>
<p><img src="https://i.loli.net/2020/12/01/DIaUG4Oz9cnEuB8.png" alt="image.png"></p>
<h6 id="典型的IA-32结构"><a href="#典型的IA-32结构" class="headerlink" title="典型的IA-32结构"></a>典型的IA-32结构</h6><p>1.8088（8位CPU：寄存器16位）/8086</p>
<p>2.80386   IA-32系列第一个32位处理器。保护模式</p>
<p>80486：第一次：指令执行采用流水线（5级），引入缓存（CACHE），引入 FPU（浮点处理单元）</p>
<p>3.奔腾（Pentium） 增加第二条流水线，达到超标量性能</p>
<p>4.Intel超线程处理器 </p>
<p>5.Intel多核处理器</p>
<h2 id="第3章-接口"><a href="#第3章-接口" class="headerlink" title="第3章 接口"></a>第3章 接口</h2><p>各种外设都必须通过接口才能和CPU（或总线）相连</p>
<p>端口：寄存器的另一称呼</p>
<p>接口电路的组成：由多类/多个寄存器构成：数据寄存器、状态寄存器、命令寄存器</p>
<p>微机的端口 16根I/O线：地址空间216 = 64K</p>
<p>PC系统IO端口的分配：前256个端口：000h-0FFh，系统外设占用</p>
<h4 id="端口访问指令"><a href="#端口访问指令" class="headerlink" title="端口访问指令"></a>端口访问指令</h4><p>端口地址（000h ~ 3FFh） </p>
<p>端口属性：只写，只读，可读可写 </p>
<p>端口操作：写(OUT指令)，读(IN指令)（AL/AX） 。</p>
<h4 id="端口地址设计-重点"><a href="#端口地址设计-重点" class="headerlink" title="端口地址设计 重点"></a>端口地址设计 重点</h4><ul>
<li><p>有效I/O地址线10位（或16位）: A9～0 (或A15 ～ 0 ) </p>
</li>
<li><p>端口读写属性（只读/只写/可读可写） </p>
</li>
<li><p>考虑DMA操作：地址允许信号AEN（DMAC信号） </p>
<p>AEN=0，即非DMA操作时，端口可以访问； </p>
<p>AEN=1，即是DMA操作时，端口不能访问；</p>
</li>
</ul>
<h5 id="含有多个端口的接口地址译码"><a href="#含有多个端口的接口地址译码" class="headerlink" title="含有多个端口的接口地址译码"></a>含有多个端口的接口地址译码</h5><p>例子：某接口有4个端口：384H~387H 。</p>
<p>仅当A9-0 = 11 1000 01XX时译码电路输出低电平。</p>
<p><img src="https://i.loli.net/2020/12/01/NIzJDKoZTYcH5xQ.png" alt="image-20201121224100450.png"></p>
<p><img src="https://i.loli.net/2020/12/01/SdZUaYNm2bcoXyq.png" alt="image-20201121224358115.png"></p>
<p>独立编址的端口访问原理：根据指令（IN/OUT|MOV）区分内存和端口。（微机和大型计算机通常采用这种方式）</p>
<p>统一编址的端口访问原理：根据地址区分内存和端口。</p>
<h4 id="数据传输方式"><a href="#数据传输方式" class="headerlink" title="数据传输方式"></a>数据传输方式</h4><h6 id="1-无条件传送（同步传送）"><a href="#1-无条件传送（同步传送）" class="headerlink" title="1.无条件传送（同步传送）"></a>1.无条件传送（同步传送）</h6><p>假定外设已经 准备就绪，直接使用I/O指令（IN或OUT）与外设传送数据。</p>
<p>接口电路不需要状态端口</p>
<h6 id="2-查询传送方式（异步传送）"><a href="#2-查询传送方式（异步传送）" class="headerlink" title="2.查询传送方式（异步传送）"></a>2.查询传送方式（异步传送）</h6><p>具有数据端口和状态端口</p>
<p>状态端口一般只需其中1位即可（D7）。当REDAY为 1 时，表明输入数据已准备好；完成数据输入后，READY自动变0。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">POLL: IN AL， PORT_State ;读状态端口：PORT_State</span><br><span class="line">TEST AL， 80H ;10000000B检查READY是否为1</span><br><span class="line">JZ POLL ;未准备好，转POLL</span><br><span class="line">IN AL， PORT_Data ;读数据端口：PORT_Data</span><br></pre></td></tr></table></figure>

<p>—输出</p>
<p>过程：读状态端口（D4 EMPTY=1？）→写数据端口</p>
<h6 id="3-中断传送方式"><a href="#3-中断传送方式" class="headerlink" title="3.中断传送方式"></a>3.中断传送方式</h6><h6 id="4-直接存储器存取方式（DMA"><a href="#4-直接存储器存取方式（DMA" class="headerlink" title="4.直接存储器存取方式（DMA)"></a>4.直接存储器存取方式（DMA)</h6><h2 id="第5章-定时器-计数器和8253A"><a href="#第5章-定时器-计数器和8253A" class="headerlink" title="第5章 定时器/计数器和8253A"></a>第5章 定时器/计数器和8253A</h2><p>软件方法  运用循环执行一段指令产生的一定时长的延时。增加CPU开销；延时依赖CPU频率；</p>
<p>硬件方法  采用专用电路（例如：定时/计数器）产生定时。不占用CPU时间；定时准确不受主机频率影响</p>
<p>定时时间 ＝ 计数数量 × 时间间隔</p>
<h5 id="8253的结构和工作原理"><a href="#8253的结构和工作原理" class="headerlink" title="8253的结构和工作原理"></a>8253的结构和工作原理</h5><ul>
<li><p>有3个16位相互独立的计数器：T0, T1, T2</p>
</li>
<li><p>每个计数器都可以按照二进制或二—十进制(BCD)计数 </p>
</li>
<li><p>每个计数器可设置6种不同的工作方式 </p>
</li>
<li><p>每个计数器可以预置计数初值（时间常数） </p>
</li>
<li><p>计数器的当前计数值可被CPU读出</p>
</li>
</ul>
<h6 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h6><p>②数据总线缓冲器。 与CPU数据总线D0~D7相连 写入命令字； 写入计数初值； 读出计数初值或当前值 </p>
<p>③控制命令寄存器。 接收控制命令，选择计数器及设定工作方式</p>
<h6 id="外部引脚"><a href="#外部引脚" class="headerlink" title="外部引脚"></a>外部引脚</h6><p>① 数据总线D0~D7  三态输出/输入线：数据、命令和状态。</p>
<p>⑤地址线A1A0 用于选择8253A内部寄存器</p>
<p>⑧计数器输出OUT 输出特定波形标识定时或计数完毕或计数过程 OUT0、OUT1、OUT2</p>
<p><img src="https://i.loli.net/2020/12/01/aHt6IkufAPmwz7W.png" alt="image-20201121232134940.png"></p>
<h6 id="计数初值C的确定"><a href="#计数初值C的确定" class="headerlink" title="计数初值C的确定"></a>计数初值C的确定</h6><p>1.单纯的计数：直接设定 </p>
<p>2.作为定时用，把时间L转成相应的计数C </p>
<p><img src="https://i.loli.net/2020/12/01/I2JpQ4bANHDPB39.png" alt="image-20201121232413991.png"></p>
<p>端口选择：4个端口：T0,T1,T2,控制口</p>
<h5 id="8253A的初始化和基本操作"><a href="#8253A的初始化和基本操作" class="headerlink" title="8253A的初始化和基本操作"></a>8253A的初始化和基本操作</h5><p><img src="https://i.loli.net/2020/12/01/OEeidLD9aVx6fb7.png" alt="image-20201121232609396.png"></p>
<h6 id="初始化：工作方式控制字"><a href="#初始化：工作方式控制字" class="headerlink" title="初始化：工作方式控制字"></a>初始化：工作方式控制字</h6><p><img src="https://i.loli.net/2020/12/01/TbHUc1ASd8jYRlI.png" alt="image-20201121232651261.png"></p>
<h6 id="基本操作：通过向控制端口写特定的字完成。"><a href="#基本操作：通过向控制端口写特定的字完成。" class="headerlink" title="基本操作：通过向控制端口写特定的字完成。"></a>基本操作：通过向控制端口写特定的字完成。</h6><h6 id="（1）获取当前计数值：直接读取或锁存命令"><a href="#（1）获取当前计数值：直接读取或锁存命令" class="headerlink" title="（1）获取当前计数值：直接读取或锁存命令"></a>（1）获取当前计数值：直接读取或锁存命令</h6><p>使用IN指令读取（两次）读数不准确    硬件配合：先禁止计数，再读数</p>
<p>锁存：</p>
<p><img src="https://i.loli.net/2020/12/01/uJDAFplPSnvtBdL.png" alt="image-20201121233345902.png"></p>
<h6 id="（2）获得工作状态：获得状态字"><a href="#（2）获得工作状态：获得状态字" class="headerlink" title="（2）获得工作状态：获得状态字"></a>（2）获得工作状态：获得状态字</h6><h5 id="8253A的工作方式和应用"><a href="#8253A的工作方式和应用" class="headerlink" title="8253A的工作方式和应用"></a>8253A的工作方式和应用</h5><ul>
<li><p>方式0和方式 1 </p>
<p>输出波形类似；</p>
<p>无自动重装C的能力； </p>
<p>启动计数的触发信号不一样： </p>
<ul>
<li><p>方式0：软件（写初值）</p>
</li>
<li><p>方式1：硬件（GATE上沿） </p>
</li>
</ul>
</li>
<li><p>方式2 （N分频器）和方式3 （方波发生器）</p>
<p>计数初值自动重装，循环计数； </p>
<p>OUT频率:CLK的N分之一; </p>
<p>方式2：计数时高电平，结束时1个CLK负脉冲； </p>
<p><img src="https://i.loli.net/2020/12/01/TbHUc1ASd8jYRlI.png" alt="image-20201121232651261.png"></p>
<p>方式3：前一半为高，后一半为低</p>
</li>
<li><p>方式4 （单次负脉冲）和5方式（单次负脉冲） </p>
<p>输出波形相同：单次负脉冲；</p>
<p>无自动重装能力； </p>
<p>启动计数方式不同：</p>
<ul>
<li>方式4：软件</li>
<li>方式5：硬件 </li>
</ul>
</li>
<li><p>方式2 （N分频）和方式4与5 </p>
<p>方式2：周期性负脉冲</p>
<p>方式4与5：单次负脉冲</p>
</li>
</ul>
<h2 id="第6章-中断和8259A"><a href="#第6章-中断和8259A" class="headerlink" title="第6章 中断和8259A"></a>第6章 中断和8259A</h2><p>中断的作用和应用  ①同步操作/并行操作 ②实时处理 ③故障或异常处理</p>
<p>中断向量是指：中断源（N）的中断服务程序的入口地址 CS:IP</p>
<p>每个向量占4字节。</p>
<p>中断向量表存放位置  0h ~ 3FFh</p>
<p>查找N号断服务程序的入口地址（即中断向量）：读取（4 * N）~ （4 * N +3）连续4个单元 ，更新CSl:IP（跳转）</p>
<h6 id="CPU能决定是否响应中断-响应中断的条件"><a href="#CPU能决定是否响应中断-响应中断的条件" class="headerlink" title="CPU能决定是否响应中断(响应中断的条件)"></a>CPU能决定是否响应中断(响应中断的条件)</h6><p>①执行完现行指令 ：最后1个总线周期最后1个状态(T4)，检测INTR引脚。 </p>
<p>②开中断状态：STI指令</p>
<p>关中断-保护断点-保护现场-中断服务-恢复现场-返回开中断</p>
<p><img src="https://i.loli.net/2020/12/01/7ewJpN8y3TgGoxj.png" alt="image-20201122221943867.png"></p>
<h6 id="识别中断源和中断优先级实现方法"><a href="#识别中断源和中断优先级实现方法" class="headerlink" title="识别中断源和中断优先级实现方法"></a>识别中断源和中断优先级实现方法</h6><ul>
<li><p>软件方法（查询中断）</p>
<p>将8个外设中断请求触发器组合为中断源寄存器</p>
<p>进入中断服务的时间较长</p>
<p>改进：用硬件实现查询和优先权管理 链式电路 本质还是查询方法，中断响应和进入较慢</p>
</li>
<li><p>硬件方法（向量中断）</p>
<p>每个外设预先指定一个中断类型码（N）。当CPU识别出 外设有请求中断并予以响应时，中断控制逻辑把N送入 CPU，CPU据此自动查询中断向量表获取其中预存的中断 服务程序的入口地址，并转入中断服务程序。</p>
</li>
</ul>
<h5 id="8259内部结构和外部引脚"><a href="#8259内部结构和外部引脚" class="headerlink" title="8259内部结构和外部引脚"></a>8259内部结构和外部引脚</h5><p><img src="https://i.loli.net/2020/12/01/k8XCOSZmyQqIv1U.png" alt="image-20201122222633546.png"></p>
<p><strong>优先权电路：实现优先级排队</strong>（<em>考过</em></p>
<h5 id="8259A的工作方式"><a href="#8259A的工作方式" class="headerlink" title="8259A的工作方式"></a>8259A的工作方式</h5><p>优先级排队的方式</p>
<p>① 全嵌套方式。常用缺省方式 。优先级按0～7顺序排队，且只允许级别高的中断源去中断级别低的中断服务程序。</p>
<p>② 自动轮换方式  中断服务结束后优先级降为最低，相邻的低优先级中断源自动升为最高，其余顺变。</p>
<h5 id="8259的编程和应用"><a href="#8259的编程和应用" class="headerlink" title="8259的编程和应用"></a>8259的编程和应用</h5><p>初始化阶段 ICW1 ～ICW4</p>
<p><img src="https://i.loli.net/2020/12/01/l953VBgNnZ2LFhU.png" alt="image-20201122223456580.png"></p>
<p>操作控制阶段 OCW1～OCW3</p>
<h5 id="8259A中断响应过程"><a href="#8259A中断响应过程" class="headerlink" title="8259A中断响应过程"></a>8259A中断响应过程</h5><p><img src="https://i.loli.net/2020/12/01/pomdKBiS83tAVEZ.png" alt="image-20201122223907130.png"></p>
<p>8259A可以级连，1个主片最多可以级连8个从片</p>
<p>主片在第1个响应周期内通过CAS2～0送出从片ID，相 应的从片在第2个响应周期内则将中断类型码N发送 到数据总线上。</p>
<p>8259A的2个端口：</p>
<p>按端口地址区分命令（偶地址A0=0和奇地址A0=1） </p>
<p>按顺序或特征位区分命令（同一端口地址）</p>
<p><img src="https://i.loli.net/2020/12/01/a6FedUuL5ot4qG1.png" alt="image-20201122224441960.png"></p>
<p>初始化中断向量表</p>
<p>前提：1片8259芯片，N=40~47. IR5连接有某外设 </p>
<p>任务：初始化8259并编写IR5的中断服务程序INTService</p>
<p>例：将中断类型码N = 45 H的服务程序入口地址放入中断向量表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MOV AX，0000H</span><br><span class="line">MOV DS，AX ；数据段从内存0地址开始（安排向量表）</span><br><span class="line">MOV SI， 114 ；中断向量的存放地址：45H x 4&#x3D;114H</span><br><span class="line">MOV BX，OFFSET INTService ;中断服务程序的偏移IP</span><br><span class="line">MOV [SI]，BX</span><br><span class="line">MOV BX，SEG INTService ;中断服务程序的段基址CS</span><br><span class="line">MOV [SI+2]，BX</span><br></pre></td></tr></table></figure>

<h2 id="第7章-直接内存存取（DMA）"><a href="#第7章-直接内存存取（DMA）" class="headerlink" title="第7章 直接内存存取（DMA）"></a>第7章 直接内存存取（DMA）</h2><h6 id="外设与内存之间的数据传送方式"><a href="#外设与内存之间的数据传送方式" class="headerlink" title="外设与内存之间的数据传送方式"></a>外设与内存之间的数据传送方式</h6><ul>
<li><p>无条件传送，查询传送，中断传送 </p>
<p>共同特点：外设 ↔ CPU ↔ 内存</p>
<p>缺点：CPU的工作效率低(CPU忙于数据传送工作)</p>
</li>
<li><p>DMA传送</p>
<p>\在内存-I/O设备或内存-内存或I/O设备-I/O设备间直接传送数据，不需要CPU的中转，相应更快</p>
</li>
</ul>
<h5 id="8237A"><a href="#8237A" class="headerlink" title="8237A"></a>8237A</h5><p>有四个独立DMA通道</p>
<p>每个通道一次传送数据的最大长度可达64K字节</p>
<p>单字节传送 | 数据块传送 | 请求传送 | 级连</p>
<p>DB0~DB7：双向三态双功能线。主动态：地址线，访问存储器的高8位地址A15-8 。</p>
<p>ADSTB：地址选通，输出。 用作16位地址的高8位地址锁存器的<em>选通</em>信号。</p>
<p>AEN：地址允许，输出。用作16位地址的高8位地址锁存器的<em>允许</em>信号。</p>
<h6 id="8237的工作周期"><a href="#8237的工作周期" class="headerlink" title="8237的工作周期"></a>8237的工作周期</h6><ul>
<li><p>空闲周期SI</p>
<p>被动态。采样DREQ    采样CS</p>
</li>
<li><p>过渡状态 S0</p>
<p>初始化后，检测到DREQ。若DREQ有效，向CPU发送HOLD信号，进入过渡状态S0。收到CPU的HLDA后，结束S0，进入有效周期的S1状态。</p>
</li>
<li><p>有效周期S1，S2，S3，S4，SW</p>
<p>S1：更新高8位地址</p>
<p>S2：寻址内存，输出16位RAM地址和发出DACK信号寻址I/O</p>
<p>S3：读周期。读源数据:发出MEMR或IOR 将8位数据放到DB等写</p>
<p>S4：写周期。写数据:发出MEMW或IOW将DB上的数据写到RAM货I/O，完成1字节传输</p>
</li>
</ul>
<h6 id="8237A工作模式"><a href="#8237A工作模式" class="headerlink" title="8237A工作模式"></a>8237A工作模式</h6><p>1.单字节传输模式  每次请求总线只传送一个字节数据，传送完后释放总线。</p>
<p>2.块传输模式  每次请求总线连续传送一个数据块，整块传完释放总线。  DREQ只需维持有效到DACK有效 </p>
<p>3.请求传输模式 每传完一个字节，检查DREQ是否仍然有效。 如果DREQ无效，就暂停传输，交还总线控制权给CPU。 当DREQ再次变为有效，传输则从<strong>暂停点</strong>继续。</p>
<p>4.级联传输模式  多片8237A，构成主从式DMA系统。 从片的HRQ端与主片的DREQ连接。</p>
<h2 id="第10章-DA转换和AD转换"><a href="#第10章-DA转换和AD转换" class="headerlink" title="第10章 DA转换和AD转换"></a>第10章 DA转换和AD转换</h2><p>ADC0809</p>
<p>EOC输出信号变低，指示A/D转换正在进行</p>
<p>EOC变高电平，指示A/D转换结束。结果存在8位锁存器中</p>
<p>OE信号变高电平，则8位锁存器的数据输出到DB上。</p>
<p>查询传送时，EOC作为ADC转换结束的状态信息可被查询。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Microcomputer/" rel="tag">Microcomputer</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-编译原理复习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/11/12/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0/"
    >编译原理复习</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/11/12/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0/" class="article-date">
  <time datetime="2020-11-12T15:13:01.000Z" itemprop="datePublished">2020-11-12</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Review/">Review</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>#复习</p>
<h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><ul>
<li>编译程序是一个将源语言转化为目标语言的计算机程序</li>
<li>简述编译程序的工作过程。（10）<br>编译程序的工作过程，是指从输入源程序开始到输出目标程序为止的整个过程，是非常复杂的，就其过程而言，一般可以划分为五个工作阶段：<br>①词法分析，对构成源程序的字符串进行扫描和分解，识别出一个个的单词；（将输入程序 字符流 分离成一个个token）<br>②语法分析，根据语言的语法规则，把单词符号串分解成各类语法单位；（分析输入的token是否符合语法规则）<br>③语义分析与中间代码产生，即对各类语法单位，分析其含义并进行初步翻译；④代码优化，以期产生更高效的代码；<br>⑤目标代码生成，把中间代码变换成特定机器上的低级语言指令形式。</li>
<li>编译器的预处理器：在编译开始之前对程序所作的预先处理，如替换宏定义，将包含文件展开等。</li>
</ul>
<h2 id="第二章-文法"><a href="#第二章-文法" class="headerlink" title="第二章 文法"></a>第二章 文法</h2><ol>
<li><strong>设计一个已知语言的文法</strong><br>被5整除、被3整除</li>
<li>已知文法求语言</li>
<li><strong>句型的短语、直接短语、句柄</strong><br>语法树 写👆 要有（相对于…）</li>
<li>文法二义性</li>
</ol>
<ul>
<li>消除方法：（2文法）增加消除二义性的规则或重写文法规则</li>
<li>二义文法生成的语言L(G)不一定是二义的。因为对于一个语言来说，可以由多个文法生成，只要有一个生成该语言的文法不是二义的，该语言就不是二义的。</li>
<li>文法的二义性，并不等同于语言的二义性，尽管两者之间可能存在非必然的联系。因为二义性文法G，可能存在与之等价的无二义性的文法 G′，即L(G)＝L(G′）。</li>
<li>如果一个语言不存在无二义性的文法，则称该语言是先天二义性的。</li>
</ul>
<ol start="5">
<li>文法类型<br>A→Aa是左线性3型文法<br>最右推导，也叫规范推导。由规范推导所得的句型，叫做规范句型。规范推导的逆过程，叫做规范归约。 </li>
</ol>
<h2 id="第三章-词法分析（大题）"><a href="#第三章-词法分析（大题）" class="headerlink" title="第三章 词法分析（大题）"></a>第三章 词法分析（大题）</h2><ol>
<li>设计一个定义已知语言单词集的正规文法、正规式、有穷自动机（NFA&amp;DFA）</li>
</ol>
<ul>
<li>一个确定的有穷自动机DFA M是一个五元组：M=(K,求和,f,S,Z)。 其中:<br>K是非空有穷集，每个元素称为状态；<br>求和是有穷字母表；<br>f是K×→K映射，称为状态转换函数；<br>S属于K，称为开始状态；<br>Z 包含于K，称为结束状态集，或接受状态集。</li>
<li>NFA和DFA的区别：NFA多开始状态、DFA唯一<br>NFA同一状态接受同一字符可能到达不同状态<br>NFA终止状态能为空？</li>
</ul>
<ol start="2">
<li>三者等价转换<br>::正规式-&gt;正规文法::<br> A→xy A→xB,B→y<br> A→x*y A→xB, A→y, B→xB, B→y<br>::闭包运算 closure&amp;move:: ::正规式到最简DFA::</li>
<li>正规式运算性质及应用</li>
<li>确定最小的有穷自动机DFA<br>::文法—&gt;NFA—&gt;DFA(子集法)—&gt;最小DFA(分割法)::<br>上下文无关文法是chomsky2型文法，表达能力等价于下推自动机，描述能力比有限自动机强。与有限自动机等价的是正规文法。</li>
</ol>
<h2 id="第四章-自顶向下分析"><a href="#第四章-自顶向下分析" class="headerlink" title="第四章 自顶向下分析"></a>第四章 自顶向下分析</h2><p>自上而下分析法：从文法开始符号出发，反复使用规则，寻找匹配符号串（推导）的句型，直到推导出句子或规则用遍。进 行每步推导时，存在两个选择问题：<br>⑴ 选择句型中哪一个非终结符进行推导<br>⑵ 选择非终结符的哪一个规则进行推导</p>
<ol>
<li>计算SELECT()、FIRST()、::FOLLOW()::</li>
<li><strong>LL（1）文法判别</strong><br>充要条件SELECT集相交为空<blockquote>
<p>求出能推出空的非终结符<br>计算FIRST、FOLLOW、SELECT  </p>
</blockquote>
</li>
<li>非LL（1）文法到LL（1）文法的等价变换<br>提取左公共因子&amp;消除左递归</li>
<li>构造LL（1）分析表<br>LL（1）预测分析法算法（表+分析过程）<br>步骤 符号串 剩余输入串 动作</li>
</ol>
<ul>
<li>LL1文法的两个主要操作：</li>
<li>匹配</li>
<li>生成</li>
</ul>
<h2 id="第六章-LR分析"><a href="#第六章-LR分析" class="headerlink" title="第六章  LR分析"></a>第六章  LR分析</h2><p>自下而上分析法：从输入符号串α开始，逐步进行“归约”，直至归约出文法的开始符号 S，则输入串α是文法G定义的语言的句子。否则不是。<br>这种分析方法在进行每步归约时，存在两个如何选择句型α的子串β进行归约的问题(α=β δ)。<br>通过在句型中寻找所谓的“句柄”的途径解决的。</p>
<ol>
<li>可递归前缀&amp;活前缀 PPT看 ::求活前缀::</li>
<li>活前缀DFA（过程！？</li>
<li>LR（0）项目集规范族 </li>
<li>四类项目：移进项目、归约、接受、 <del>GOTO</del>待约<br>四种动作：移进 归约 接受 报错</li>
<li>LR（0）、SLR（1）、LR（1）文法判别 分析表和算法实现</li>
</ol>
<ul>
<li>自底向上解析的两种主要操作：</li>
<li>移进</li>
<li>归约</li>
</ul>
<h2 id="第七章-语法导致的语义计算"><a href="#第七章-语法导致的语义计算" class="headerlink" title="第七章 语法导致的语义计算"></a>第七章 语法导致的语义计算</h2><ol>
<li>继承属性&amp;综合属性</li>
<li>属性值传递过程（遍历语法树）</li>
<li>S-属性文法、L-属性文法</li>
<li>S-翻译 L-翻译</li>
<li>中间代码：ast、四元式</li>
</ol>
<h2 id="第八章-静态语义分析和中间代码生成"><a href="#第八章-静态语义分析和中间代码生成" class="headerlink" title="第八章 静态语义分析和中间代码生成"></a>第八章 静态语义分析和中间代码生成</h2><ol>
<li>符号表的作用</li>
</ol>
<ul>
<li>符号表的每一项是由名字栏和地址分配 两个栏目组成。在目标代码生成阶段，符号表是地址分配 的依据。   </li>
<li>一个过程的DISPLAY表的内容是它的直接外层的DISPLAY表的内容加上本过程的SP的地址</li>
</ul>
<ol start="2">
<li>作用域、开/闭的概念&amp;判断</li>
<li>静态语义分析的主要任务</li>
<li>常见的中间表示形式</li>
<li>ast创建</li>
<li>TAC表示（四元式）</li>
</ol>
<h2 id="第九章-运行时存储组织"><a href="#第九章-运行时存储组织" class="headerlink" title="第九章 运行时存储组织"></a>第九章 运行时存储组织</h2><ol>
<li>作用与任务<br>合理安排逻辑地址空间存放各种对象以及代码。编译程序生成的代码大小通常是固定的，一般存放在专用的区域，即代码区；目标程序运行过程中，需要创建和访问的数据对象存放在数据区。</li>
<li>空间布局<br>逻辑上：代码区、数据区<br>实际：保留区、代码区、静态数据、共享库和分别编译 模块、动态数据（堆、栈）</li>
<li>存储分配策略<br>静态 栈 堆</li>
</ol>
<h2 id="第十章-代码优化-amp-目标代码生成"><a href="#第十章-代码优化-amp-目标代码生成" class="headerlink" title="第十章 代码优化&amp;目标代码生成"></a>第十章 代码优化&amp;目标代码生成</h2><ol>
<li>优化技术介绍</li>
<li>机器无关优化划分：全局，局部（删除多余运算、合并已知量、复写传播、删除无用赋值），循环（代码外提、运算强度削弱、变换控制条件）</li>
</ol>
<ul>
<li>基本块：只有一个入口语句和一个出口语句的顺序程序段</li>
<li>入口语句是指程序的第一个语句、或者转移语句（包括条件转移语句和无条件转移语句）转移到的目标语句、或者 紧跟在条件转移语句之后的语句。</li>
<li>入口结点：如果在结点序列之外存在一个结点指向结点序列中的结点V，或者结点序列中的结点V 是程序首结点，则称结点V 为结点序列α的入口结点。</li>
<li>必经结点集：前驱结点的必经结点集的交集再并上自己::题::</li>
<li>循环查找算法（P(n)为结点n的所有前驱结点集）<br>(令流图G的一条回边n→m，求m为入口和n为出口之循环loop。)<br>⑴ loop←{m,n}，S←{n}；<br>⑵ S←(  ∪  P(q)| q∈S )－loop；<br>⑶ loop←loop ∪ S；<br>⑷ 重复⑵、⑶，直到所有loop不再变化为止。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Compile/" rel="tag">Compile</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020
        <i class="ri-heart-fill heart_icon"></i> Xinyuan Liu
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/chick.svg" alt="Dream"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->


<script src="/js/clickBoom2.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>